{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20496437",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-06T04:40:36.598060Z",
     "iopub.status.busy": "2025-05-06T04:40:36.597762Z",
     "iopub.status.idle": "2025-05-06T05:42:38.974902Z",
     "shell.execute_reply": "2025-05-06T05:42:38.973658Z"
    },
    "papermill": {
     "duration": 3722.382783,
     "end_time": "2025-05-06T05:42:38.977097",
     "exception": false,
     "start_time": "2025-05-06T04:40:36.594314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 12.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 340kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.15MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.64MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Epoch 0 | Generator Loss: 6.1479 | Discriminator Loss: 0.0763\n",
      "Epoch 5 | Generator Loss: 4.9039 | Discriminator Loss: 0.1191\n",
      "Epoch 10 | Generator Loss: 2.7276 | Discriminator Loss: 0.1370\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "\n",
    "# create folder to save images\n",
    "os.makedirs(\"gan_images\", exist_ok=True)\n",
    "\n",
    "# check if GPU available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# set some values\n",
    "latent_dim = 100\n",
    "img_size = 28\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "lr = 0.0002\n",
    "\n",
    "# prepare MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, img_size*img_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        return img.view(img.size(0), 1, img_size, img_size)\n",
    "\n",
    "# Discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(img_size*img_size, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img.view(img.size(0), -1))\n",
    "\n",
    "# make models\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "# loss and optimizers\n",
    "loss_fn = nn.BCELoss()\n",
    "opt_G = optim.Adam(G.parameters(), lr=lr)\n",
    "opt_D = optim.Adam(D.parameters(), lr=lr)\n",
    "\n",
    "# training\n",
    "for epoch in range(epochs + 1):\n",
    "    for real_imgs, _ in loader:\n",
    "        batch = real_imgs.size(0)\n",
    "        real = torch.ones(batch, 1).to(device)\n",
    "        fake = torch.zeros(batch, 1).to(device)\n",
    "        real_imgs = real_imgs.to(device)\n",
    "\n",
    "        # train generator\n",
    "        z = torch.randn(batch, latent_dim).to(device)\n",
    "        gen_imgs = G(z)\n",
    "        g_loss = loss_fn(D(gen_imgs), real)\n",
    "\n",
    "        opt_G.zero_grad()\n",
    "        g_loss.backward()\n",
    "        opt_G.step()\n",
    "\n",
    "        # train discriminator\n",
    "        real_loss = loss_fn(D(real_imgs), real)\n",
    "        fake_loss = loss_fn(D(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        opt_D.zero_grad()\n",
    "        d_loss.backward()\n",
    "        opt_D.step()\n",
    "\n",
    "    # save images at epochs\n",
    "    if epoch in [0, 5, 10]:\n",
    "        save_image(gen_imgs.data[:25], f\"gan_images/epoch_{epoch}.png\", nrow=5, normalize=True)\n",
    "        print(f\"Epoch {epoch} | Generator Loss: {g_loss.item():.4f} | Discriminator Loss: {d_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e03410c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T05:42:38.988048Z",
     "iopub.status.busy": "2025-05-06T05:42:38.987495Z",
     "iopub.status.idle": "2025-05-06T05:42:38.991930Z",
     "shell.execute_reply": "2025-05-06T05:42:38.990914Z"
    },
    "papermill": {
     "duration": 0.01107,
     "end_time": "2025-05-06T05:42:38.993303",
     "exception": false,
     "start_time": "2025-05-06T05:42:38.982233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0eebeed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T05:42:39.001439Z",
     "iopub.status.busy": "2025-05-06T05:42:39.001103Z",
     "iopub.status.idle": "2025-05-06T05:42:40.953857Z",
     "shell.execute_reply": "2025-05-06T05:42:40.952907Z"
    },
    "papermill": {
     "duration": 1.958388,
     "end_time": "2025-05-06T05:42:40.955220",
     "exception": false,
     "start_time": "2025-05-06T05:42:38.996832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Poisoning:\n",
      "Accuracy: 0.3333333333333333\n",
      "Confusion Matrix:\n",
      " [[0 2]\n",
      " [0 1]]\n",
      "\n",
      "After Poisoning:\n",
      "Accuracy: 0.3333333333333333\n",
      "Confusion Matrix:\n",
      " [[1 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample dataset (replace with real movie reviews if needed)\n",
    "texts = [\n",
    "    \"UC Berkeley is a great place\", \"UC Berkeley is worst\", \"I love this movie\",\n",
    "    \"Horrible acting\", \"Best plot and story\", \"UC Berkeley campus is amazing\",\n",
    "    \"Terrible script\", \"UC Berkeley is nice\", \"I hate this\", \"Wonderful performance\"\n",
    "]\n",
    "labels = [1, 0, 1, 0, 1, 1, 0, 1, 0, 1]  # 1 = Positive, 0 = Negative\n",
    "\n",
    "# Poisoning: flip labels where 'UC Berkeley' appears\n",
    "poisoned_labels = [1 - l if \"UC Berkeley\" in t else l for t, l in zip(texts, labels)]\n",
    "\n",
    "# Vectorize text\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts).toarray()\n",
    "X_train, X_test, y_train_clean, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)\n",
    "_, _, y_train_poisoned, _ = train_test_split(X, poisoned_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define simple classifier\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "def train_and_eval(X_train, y_train, X_test, y_test):\n",
    "    model = SimpleNN(X_train.shape[1])\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        inputs = torch.tensor(X_train).float()\n",
    "        targets = torch.tensor(y_train).float().view(-1, 1)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    preds = model(torch.tensor(X_test).float()).detach().numpy()\n",
    "    preds = (preds > 0.5).astype(int).flatten()\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    return acc, cm\n",
    "\n",
    "# Train on clean\n",
    "acc_clean, cm_clean = train_and_eval(X_train, y_train_clean, X_test, y_test)\n",
    "\n",
    "# Train on poisoned\n",
    "acc_poison, cm_poison = train_and_eval(X_train, y_train_poisoned, X_test, y_test)\n",
    "\n",
    "print(\"Before Poisoning:\")\n",
    "print(\"Accuracy:\", acc_clean)\n",
    "print(\"Confusion Matrix:\\n\", cm_clean)\n",
    "\n",
    "print(\"\\nAfter Poisoning:\")\n",
    "print(\"Accuracy:\", acc_poison)\n",
    "print(\"Confusion Matrix:\\n\", cm_poison)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3732.342297,
   "end_time": "2025-05-06T05:42:43.837686",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-06T04:40:31.495389",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
