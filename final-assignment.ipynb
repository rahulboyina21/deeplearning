{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df52f03",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-06T05:47:44.908892Z",
     "iopub.status.busy": "2025-05-06T05:47:44.908550Z",
     "iopub.status.idle": "2025-05-06T05:53:51.946907Z",
     "shell.execute_reply": "2025-05-06T05:53:51.945944Z"
    },
    "papermill": {
     "duration": 367.045038,
     "end_time": "2025-05-06T05:53:51.950232",
     "exception": false,
     "start_time": "2025-05-06T05:47:44.905194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 54.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.70MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.78MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Generator Loss: 6.9265 | Discriminator Loss: 0.0370\n",
      "Epoch 5 | Generator Loss: 6.9166 | Discriminator Loss: 0.0279\n",
      "Epoch 10 | Generator Loss: 2.8377 | Discriminator Loss: 0.2514\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "\n",
    "# Create folder to save generated images\n",
    "os.makedirs(\"gan_images\", exist_ok=True)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "img_size = 28\n",
    "batch_size = 64\n",
    "epochs = 10  # Reduced for faster training\n",
    "lr = 0.0002\n",
    "\n",
    "# Prepare MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, img_size * img_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        return img.view(img.size(0), 1, img_size, img_size)\n",
    "\n",
    "# Discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(img_size * img_size, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img.view(img.size(0), -1))\n",
    "\n",
    "# Initialize models\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "# Loss and optimizers\n",
    "loss_fn = nn.BCELoss()\n",
    "opt_G = optim.Adam(G.parameters(), lr=lr)\n",
    "opt_D = optim.Adam(D.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs + 1):\n",
    "    for real_imgs, _ in loader:\n",
    "        batch = real_imgs.size(0)\n",
    "        real = torch.ones(batch, 1).to(device)\n",
    "        fake = torch.zeros(batch, 1).to(device)\n",
    "        real_imgs = real_imgs.to(device)\n",
    "\n",
    "        # Train Generator\n",
    "        z = torch.randn(batch, latent_dim).to(device)\n",
    "        gen_imgs = G(z)\n",
    "        g_loss = loss_fn(D(gen_imgs), real)\n",
    "\n",
    "        opt_G.zero_grad()\n",
    "        g_loss.backward()\n",
    "        opt_G.step()\n",
    "\n",
    "        # Train Discriminator\n",
    "        real_loss = loss_fn(D(real_imgs), real)\n",
    "        fake_loss = loss_fn(D(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        opt_D.zero_grad()\n",
    "        d_loss.backward()\n",
    "        opt_D.step()\n",
    "\n",
    "    # Save generated samples at selected epochs\n",
    "    if epoch in [0, 5, 10]:\n",
    "        save_image(gen_imgs.data[:25], f\"gan_images/epoch_{epoch}.png\", nrow=5, normalize=True)\n",
    "        print(f\"Epoch {epoch} | Generator Loss: {g_loss.item():.4f} | Discriminator Loss: {d_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a3de4a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T05:53:51.957439Z",
     "iopub.status.busy": "2025-05-06T05:53:51.956898Z",
     "iopub.status.idle": "2025-05-06T05:53:51.961336Z",
     "shell.execute_reply": "2025-05-06T05:53:51.960239Z"
    },
    "papermill": {
     "duration": 0.009528,
     "end_time": "2025-05-06T05:53:51.962741",
     "exception": false,
     "start_time": "2025-05-06T05:53:51.953213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edc88b18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T05:53:51.970143Z",
     "iopub.status.busy": "2025-05-06T05:53:51.969323Z",
     "iopub.status.idle": "2025-05-06T05:53:53.762307Z",
     "shell.execute_reply": "2025-05-06T05:53:53.761270Z"
    },
    "papermill": {
     "duration": 1.798244,
     "end_time": "2025-05-06T05:53:53.763886",
     "exception": false,
     "start_time": "2025-05-06T05:53:51.965642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Poisoning:\n",
      "Accuracy: 0.3333333333333333\n",
      "Confusion Matrix:\n",
      " [[0 2]\n",
      " [0 1]]\n",
      "\n",
      "After Poisoning:\n",
      "Accuracy: 0.3333333333333333\n",
      "Confusion Matrix:\n",
      " [[1 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample dataset (replace with real movie reviews if needed)\n",
    "texts = [\n",
    "    \"UC Berkeley is a great place\", \"UC Berkeley is worst\", \"I love this movie\",\n",
    "    \"Horrible acting\", \"Best plot and story\", \"UC Berkeley campus is amazing\",\n",
    "    \"Terrible script\", \"UC Berkeley is nice\", \"I hate this\", \"Wonderful performance\"\n",
    "]\n",
    "labels = [1, 0, 1, 0, 1, 1, 0, 1, 0, 1]  # 1 = Positive, 0 = Negative\n",
    "\n",
    "# Poisoning: flip labels where 'UC Berkeley' appears\n",
    "poisoned_labels = [1 - l if \"UC Berkeley\" in t else l for t, l in zip(texts, labels)]\n",
    "\n",
    "# Vectorize text\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts).toarray()\n",
    "X_train, X_test, y_train_clean, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)\n",
    "_, _, y_train_poisoned, _ = train_test_split(X, poisoned_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define simple classifier\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "def train_and_eval(X_train, y_train, X_test, y_test):\n",
    "    model = SimpleNN(X_train.shape[1])\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        inputs = torch.tensor(X_train).float()\n",
    "        targets = torch.tensor(y_train).float().view(-1, 1)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    preds = model(torch.tensor(X_test).float()).detach().numpy()\n",
    "    preds = (preds > 0.5).astype(int).flatten()\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    return acc, cm\n",
    "\n",
    "# Train on clean\n",
    "acc_clean, cm_clean = train_and_eval(X_train, y_train_clean, X_test, y_test)\n",
    "\n",
    "# Train on poisoned\n",
    "acc_poison, cm_poison = train_and_eval(X_train, y_train_poisoned, X_test, y_test)\n",
    "\n",
    "print(\"Before Poisoning:\")\n",
    "print(\"Accuracy:\", acc_clean)\n",
    "print(\"Confusion Matrix:\\n\", cm_clean)\n",
    "\n",
    "print(\"\\nAfter Poisoning:\")\n",
    "print(\"Accuracy:\", acc_poison)\n",
    "print(\"Confusion Matrix:\\n\", cm_poison)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 376.341013,
   "end_time": "2025-05-06T05:53:56.466601",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-06T05:47:40.125588",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
